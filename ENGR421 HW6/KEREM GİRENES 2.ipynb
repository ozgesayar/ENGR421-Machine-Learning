{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 06: Support Vector Machine Classification\n",
    "## Kerem Girenes\n",
    "### April 28, 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.spatial.distance as dt\n",
    "import scipy\n",
    "#from qpsolvers import solve_qp\n",
    "#import ecos\n",
    "import cvxopt as cvx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.genfromtxt(\"hw06_data_set_images.csv\", delimiter=\",\")\n",
    "labels = np.genfromtxt(\"hw06_data_set_labels.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividing the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = images[:1000]\n",
    "X_test = images[1000:]\n",
    "y_train = labels[:1000].astype(int)\n",
    "y_test = labels[1000:].astype(int)\n",
    "\n",
    "# two distinct classes\n",
    "K = 2\n",
    "\n",
    "# N_train = N_test\n",
    "N = X_train.shape[0]\n",
    "\n",
    "# Pixel Count\n",
    "PX = X_train.shape[1]\n",
    "\n",
    "# Number of Bins\n",
    "NO_BINS = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Color Histograms for Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.86479592 0.00127551 0.         0.00255102 0.        ]\n",
      " [0.66836735 0.         0.00127551 0.00127551 0.        ]\n",
      " [0.66454082 0.00637755 0.00382653 0.00765306 0.00892857]\n",
      " [0.65816327 0.00765306 0.00892857 0.00127551 0.00382653]\n",
      " [0.5625     0.00255102 0.00255102 0.00127551 0.        ]]\n",
      "[[0.68239796 0.00255102 0.00127551 0.00127551 0.00127551]\n",
      " [0.69770408 0.01658163 0.00510204 0.00382653 0.01020408]\n",
      " [0.73341837 0.02678571 0.01530612 0.00510204 0.00637755]\n",
      " [0.63903061 0.00892857 0.00255102 0.00127551 0.        ]\n",
      " [0.75382653 0.00765306 0.00127551 0.00127551 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "bins = np.linspace(0, 1, NO_BINS)\n",
    "\n",
    "H_train = np.zeros((N, NO_BINS))\n",
    "H_test = np.zeros((N, NO_BINS))\n",
    "\n",
    "for xi in range(N):\n",
    "    for pi in range(PX):\n",
    "        # train histogram\n",
    "        b = (X_train[xi][pi]/4).astype(int)\n",
    "        H_train[xi][b] += 1\n",
    "        # test histogram\n",
    "        b = (X_test[xi][pi]/4).astype(int)\n",
    "        H_test[xi][b] += 1\n",
    "    # normalize histograms\n",
    "    H_train[xi] = H_train[xi] / PX\n",
    "    H_test[xi] = H_test[xi] / PX\n",
    "    \n",
    "print(H_train[0:5, 0:5])\n",
    "print(H_test[0:5, 0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Kernel Matrices for Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.72321429 0.77040816 0.75382653 0.62755102]\n",
      " [0.72321429 1.         0.73086735 0.78571429 0.68622449]\n",
      " [0.77040816 0.73086735 1.         0.84056122 0.70153061]\n",
      " [0.75382653 0.78571429 0.84056122 1.         0.76403061]\n",
      " [0.62755102 0.68622449 0.70153061 0.76403061 1.        ]]\n",
      "[[0.77806122 0.80867347 0.82142857 0.88647959 0.79209184]\n",
      " [0.79464286 0.76403061 0.84566327 0.86607143 0.77933673]\n",
      " [0.8380102  0.74362245 0.85714286 0.83035714 0.68877551]\n",
      " [0.71556122 0.84438776 0.75       0.83418367 0.75765306]\n",
      " [0.84438776 0.76785714 0.82397959 0.84183673 0.73469388]]\n"
     ]
    }
   ],
   "source": [
    "def h_kernel(hi, hj):\n",
    "    res = 0\n",
    "    for l in range(NO_BINS):\n",
    "        res += min(hi[l], hj[l])\n",
    "    return res\n",
    "\n",
    "K_train = np.zeros((N, N))\n",
    "K_test = np.zeros((N, N))\n",
    "\n",
    "for i in range(N):\n",
    "    for j in range(N):\n",
    "        K_train[i][j] = h_kernel(H_train[i], H_train[j])\n",
    "        K_test[i][j] = h_kernel(H_test[i], H_train[j])\n",
    "        \n",
    "print(K_train[0:5, 0:5])\n",
    "print(K_test[0:5, 0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Train a SVM Classifier and Calculate the Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1. -1. -1.  1.]\n"
     ]
    }
   ],
   "source": [
    "a = [np.array([[1.0001], [-1.0001]]), np.array([[-1.0001], [1.0001]])]\n",
    "rounded = []\n",
    "\n",
    "for array in a:\n",
    "    for elt in array:\n",
    "        rounded.append((round(elt[0])))\n",
    "        \n",
    "print(np.array(rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -5.4945e+03 -6.4945e+04  7e+04  1e-01  2e-15\n",
      " 1: -6.1249e+03 -1.0540e+04  4e+03  7e-03  2e-15\n",
      " 2: -9.9525e+03 -1.0310e+04  4e+02  2e-04  1e-16\n",
      " 3: -9.9896e+03 -9.9932e+03  4e+00  2e-06  2e-16\n",
      " 4: -9.9900e+03 -9.9900e+03  4e-02  2e-08  1e-16\n",
      " 5: -9.9900e+03 -9.9900e+03  4e-04  2e-10  2e-16\n",
      "Optimal solution found.\n",
      "(1000,)\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan]\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# define Gaussian kernel function\n",
    "def gaussian_kernel(X1, X2, s):\n",
    "    D = dt.cdist(X1, X2)\n",
    "    K = np.exp(-D**2 / (2 * s**2))\n",
    "    return(K)\n",
    "\n",
    "N= len(y_train)\n",
    "def train_svm(X_set, y_set, N, s, C, epsilon):\n",
    "    # calculate Gaussian kernel\n",
    "    K_set = gaussian_kernel(X_set, X_set, s)\n",
    "    yyK = np.matmul(y_set[:,None], y_set[None,:]) * K_set\n",
    "\n",
    "    P = cvx.matrix(yyK)\n",
    "    q = cvx.matrix(-np.ones((N, 1)))\n",
    "    G = cvx.matrix(np.vstack((-np.eye(N), np.eye(N))))\n",
    "    h = cvx.matrix(np.vstack((np.zeros((N, 1)), C * np.ones((N, 1)))))\n",
    "    A = cvx.matrix(1.0 * y_set[None,:])\n",
    "    b = cvx.matrix(0.0)\n",
    "\n",
    "    # use cvxopt library to solve QP problems\n",
    "    result = cvx.solvers.qp(P, q, G, h, A, b)\n",
    "    alpha = np.reshape(result[\"x\"], N)\n",
    "    alpha[alpha < C * epsilon] = 0\n",
    "    alpha[alpha > C * (1 - epsilon)] = C\n",
    "\n",
    "    # find bias parameter\n",
    "    support_indices, = np.where(alpha != 0)\n",
    "    active_indices, = np.where(np.logical_and(alpha != 0, alpha < C))\n",
    "    w0 = np.mean(y_set[active_indices] * (1 - np.matmul(yyK[np.ix_(active_indices, support_indices)], alpha[support_indices])))\n",
    "    \n",
    "    return alpha, w0\n",
    "\n",
    "def pred(K_train, K_test, s, C, epsilon):\n",
    "    f_pred_train = []\n",
    "    f_pred_test = []\n",
    "    y_set = np.zeros((N))\n",
    "    for k in range(N):\n",
    "        if(y_train[i]==1):\n",
    "            y_set[i]=1\n",
    "        elif(y_train[i]==-1):\n",
    "            y_set[i]=1\n",
    "    alpha, w0 = train_svm(X_train, y_set, N, s, C, epsilon)\n",
    "        # training predictions\n",
    "    f_pred_train.append(np.matmul(K_train, y_set[:, None] * alpha[:, None]) + w0)\n",
    "        # test predictions\n",
    "    f_pred_test.append(np.matmul(K_test, y_set[:, None] * alpha[:, None]) + w0)\n",
    "    # prediction labels\n",
    "    y_pred_train = []\n",
    "    y_pred_test = []\n",
    "    for array in f_pred_train:\n",
    "        for elt in array:\n",
    "            y_pred_train.append((round(elt[0])))\n",
    "    for array in f_pred_test:\n",
    "        for elt in array:\n",
    "            y_pred_test.append((round(elt[0])))\n",
    "    y_pred_train = np.array(y_pred_train)\n",
    "    y_pred_test = np.array(y_pred_test) \n",
    "    print(y_pred_train.shape)    \n",
    "    return y_pred_train, y_pred_test\n",
    "    \n",
    "\n",
    "C = 10\n",
    "s = 10\n",
    "epsilon = 0.001\n",
    "\n",
    "K_train = gaussian_kernel(X_train, X_train, s)\n",
    "K_test = gaussian_kernel(X_test, X_train, s)\n",
    "\n",
    "y_pred_train, y_pred_test = pred(K_train, K_test, s, C, epsilon)\n",
    "\n",
    "print(y_pred_train)  \n",
    "print(pd.crosstab(y_pred_train, y_train, rownames=['y_predicted'], colnames=['y_train']))\n",
    "print(pd.crosstab(y_pred_test, y_test, rownames=['y_predicted'], colnames=['y_test']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train SVM by Varying C, Plot Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -5.4945e+02 -1.9994e+02  4e+03  2e+01  1e-16\n",
      " 1: -8.4922e+01 -1.9216e+02  1e+02  2e-01  2e-16\n",
      " 2: -9.2053e+01 -1.0083e+02  9e+00  8e-03  1e-16\n",
      " 3: -9.9822e+01 -9.9916e+01  1e-01  9e-05  1e-17\n",
      " 4: -9.9899e+01 -9.9900e+01  1e-03  9e-07  4e-17\n",
      " 5: -9.9900e+01 -9.9900e+01  1e-05  9e-09  2e-16\n",
      "Optimal solution found.\n",
      "(1000,)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -6.5746e+02 -6.3225e+02  4e+03  6e+00  4e-18\n",
      " 1: -2.5970e+02 -5.5771e+02  3e+02  6e-02  4e-16\n",
      " 2: -2.9845e+02 -3.1835e+02  2e+01  4e-03  1e-17\n",
      " 3: -3.1574e+02 -3.1594e+02  2e-01  4e-05  7e-18\n",
      " 4: -3.1591e+02 -3.1591e+02  2e-03  4e-07  1e-16\n",
      " 5: -3.1591e+02 -3.1591e+02  2e-05  4e-09  6e-16\n",
      "Optimal solution found.\n",
      "(1000,)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -9.9900e+02 -1.9990e+03  4e+03  1e+00  0e+00\n",
      " 1: -8.2969e+02 -1.3590e+03  5e+02  1e-02  2e-16\n",
      " 2: -9.8075e+02 -1.0026e+03  2e+01  6e-04  2e-16\n",
      " 3: -9.9882e+02 -9.9904e+02  2e-01  6e-06  1e-16\n",
      " 4: -9.9900e+02 -9.9900e+02  2e-03  6e-08  1e-17\n",
      " 5: -9.9900e+02 -9.9900e+02  2e-05  6e-10  4e-16\n",
      "Optimal solution found.\n",
      "(1000,)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -2.0791e+03 -9.7368e+03  1e+04  4e-01  4e-16\n",
      " 1: -2.3169e+03 -3.5531e+03  1e+03  4e-03  4e-16\n",
      " 2: -3.1508e+03 -3.1920e+03  4e+01  9e-05  1e-16\n",
      " 3: -3.1590e+03 -3.1594e+03  4e-01  1e-06  0e+00\n",
      " 4: -3.1591e+03 -3.1591e+03  4e-03  1e-08  6e-17\n",
      " 5: -3.1591e+03 -3.1591e+03  4e-05  1e-10  2e-16\n",
      "Optimal solution found.\n",
      "(1000,)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -5.4945e+03 -6.4945e+04  7e+04  1e-01  2e-15\n",
      " 1: -6.1249e+03 -1.0540e+04  4e+03  7e-03  2e-15\n",
      " 2: -9.9525e+03 -1.0310e+04  4e+02  2e-04  1e-16\n",
      " 3: -9.9896e+03 -9.9932e+03  4e+00  2e-06  2e-16\n",
      " 4: -9.9900e+03 -9.9900e+03  4e-02  2e-08  1e-16\n",
      " 5: -9.9900e+03 -9.9900e+03  4e-04  2e-10  2e-16\n",
      "Optimal solution found.\n",
      "(1000,)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.6295e+04 -5.4692e+05  6e+05  4e-02  4e-15\n",
      " 1: -1.7145e+04 -3.6745e+04  2e+04  1e-03  9e-16\n",
      " 2: -3.1448e+04 -3.3775e+04  2e+03  9e-05  3e-16\n",
      " 3: -3.1589e+04 -3.1613e+04  2e+01  1e-06  8e-16\n",
      " 4: -3.1591e+04 -3.1591e+04  2e-01  1e-08  8e-16\n",
      " 5: -3.1591e+04 -3.1591e+04  2e-03  1e-10  4e-16\n",
      "Optimal solution found.\n",
      "(1000,)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -5.0449e+04 -5.1450e+06  5e+06  1e-02  1e-14\n",
      " 1: -5.1391e+04 -1.5035e+05  1e+05  3e-04  7e-15\n",
      " 2: -9.1038e+04 -1.0041e+05  9e+03  2e-05  1e-15\n",
      " 3: -9.9811e+04 -9.9913e+04  1e+02  2e-07  2e-16\n",
      " 4: -9.9899e+04 -9.9900e+04  1e+00  2e-09  1e-16\n",
      " 5: -9.9900e+04 -9.9900e+04  1e-02  2e-11  2e-16\n",
      "Optimal solution found.\n",
      "(1000,)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.5846e+05 -5.0424e+07  5e+07  4e-03  6e-14\n",
      " 1: -1.5943e+05 -8.1700e+05  7e+05  6e-05  6e-15\n",
      " 2: -2.2206e+05 -3.2092e+05  1e+05  8e-06  3e-16\n",
      " 3: -3.1497e+05 -3.1850e+05  4e+03  9e-08  7e-15\n",
      " 4: -3.1590e+05 -3.1594e+05  4e+01  9e-10  1e-14\n",
      " 5: -3.1591e+05 -3.1591e+05  4e-01  9e-12  0e+00\n",
      " 6: -3.1591e+05 -3.1591e+05  4e-03  9e-14  7e-15\n",
      "Optimal solution found.\n",
      "(1000,)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -5.0000e+05 -5.0100e+08  5e+08  1e-03  2e-13\n",
      " 1: -5.0098e+05 -5.9990e+06  5e+06  2e-05  5e-14\n",
      " 2: -5.8370e+05 -1.0490e+06  5e+05  1e-06  8e-15\n",
      " 3: -9.9485e+05 -1.0415e+06  5e+04  5e-08  1e-13\n",
      " 4: -9.9895e+05 -9.9943e+05  5e+02  5e-10  6e-14\n",
      " 5: -9.9900e+05 -9.9900e+05  5e+00  5e-12  6e-14\n",
      " 6: -9.9900e+05 -9.9900e+05  5e-02  5e-14  2e-16\n",
      "Optimal solution found.\n",
      "(1000,)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-5b353f2e2c43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"-ob\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmarkersize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'training'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"-or\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmarkersize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Regularization parameter (C)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_list' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "C = [10**-1, 10**-.5, 10**0, 10**.5, 10**1, 10**1.5, 10**2, 10**2.5, 10**3]\n",
    "\n",
    "training_scores = []\n",
    "test_scores = []\n",
    "\n",
    "def accuracy(y_pred, y_truth):\n",
    "    score = 0.0\n",
    "    for i in range(len(y_truth)):\n",
    "        score += (y_pred[i] == y_truth[i]) * 1.0\n",
    "    return score / len(y_truth)\n",
    "\n",
    "for c in C:\n",
    "    y_pred_train, y_pred_test = pred(K_train, K_test, s, c, epsilon)\n",
    "    training_scores.append(accuracy(y_pred_train, y_train))\n",
    "    test_scores.append(accuracy(y_pred_test, y_test))\n",
    "    \n",
    "plt.figure(figsize=(20,6))\n",
    "plt.plot(C, train_list, \"-ob\", markersize=4, label='training')\n",
    "plt.plot(C, test_list, \"-or\", markersize=4, label='test')\n",
    "plt.xlabel(\"Regularization parameter (C)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
